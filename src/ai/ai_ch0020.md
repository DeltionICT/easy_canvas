---
title: Verdieping AI
date: 2025-09-25
---

::: section
### Verdieping AI
In dit gedeelte ga je dieper in op het programmeren met AI.

<img src="https://static.edutorial.nl/ai_engineer/ai_engineer.webp" eleventy:widths="900" alt="Afbeelding AI Engineer">


#### Rol
* Wat doet een AI Engineer

#### Technieken:
* Structured output
* Rag
* MCP

:::
::: section
### AI Engineer

#### Wat doet een AI Engineer?
- **AI gebruiken in software** → Denk aan gezichtsherkenning in een app of een chatbot.  
- **Bestaande AI-modellen koppelen** → Bijvoorbeeld via OpenAI (ChatGPT), Google AI of andere systemen.  
- **AI sneller en slimmer maken** → Zorgen dat AI niet te veel geheugen gebruikt en goed werkt.  
- **AI automatiseren** → Taken laten uitvoeren zonder dat iemand het handmatig hoeft te doen.  
- **Samenwerken met programmeurs** → Werken in een team om AI-oplossingen te bouwen.  

Het is een **creatief en technisch beroep** waarin je AI toepast om slimme software te maken.

#### Full Stack vs. Data Scientist
*   **Full-Stack Developer:** Dit is de **alleskunner**. Hij of zij bouwt zowel de voorkant (het scherm dat je ziet) als de achterkant (de server die alles laat werken) van de app. Ze zorgen ervoor dat de app er goed uitziet en goed werkt.
*   **Data Scientist:** Dit is de **data-analist**. Ze duiken in grote hoeveelheden data om patronen en trends te vinden. Ze gebruiken wiskunde en statistiek om bijvoorbeeld te voorspellen wat klanten willen of om problemen op te sporen.
*   **AI Engineer:** Dit is de **nieuwe speler** in het team. Ze gebruiken **slimme AI-modellen** (die al door anderen zijn gemaakt) om apps nóg slimmer te maken. Ze hoeven niet per se zelf AI-modellen te bouwen, maar ze weten wel hoe ze deze modellen in een app kunnen stoppen.

**Het belangrijkste verschil:**

*   De full-stack developer bouwt de **basis** van de app.
*   De data scientist analyseert **data** en maakt voorspellingen.
*   De AI engineer maakt de app **intelligent** door slimme AI-modellen te gebruiken.

:::

::: section
### AI Tools

*   **Python**: Programmeertaal (zie ook cursus [Python](https://www.edutorial.nl/python/introductie/))
*   **Pydantic**: Een data validatie library.
*   **Python-dotenv**: Zorgt ervoor dat gevoelige informatie zoals API keys veilig buiten de versiebeheeropslagplaats blijft.
*   **Streamlit**: Een Python-framework waarmee je snel interactieve webapplicaties kunt bouwen voor AI. Het maakt het eenvoudig om Python-code om te zetten in een visuele interface.
*   **FastAPI**: Een library voor het bouwen van API's, vooral handig vanwege de integratie met Pydantic.
*   **Celery**: Voor het bouwen van task queues om werk te verdelen over meerdere threads of machines.
*   **PostgreSQL**: Een SQL database. (zie ook cursus [Database queries](https://deltionict.github.io/easy_canvas/dbq/introductie/) en [Database ontwerp](https://www.edutorial.nl/dbo/introductie/))
*   **[Timescale](https://www.timescale.com/)**: Een extensie op PostgreSQL voor vector-opslag (ai-toepassingen)
*   **Pycopg**: Python library voor PostgreSQL.
*   **SQLAlchemy**: Vereenvoudigt operaties met SQL databases zoals PostgreSQL.
*   **Alembic**: Beheert database migraties in combinatie met SQLAlchemy.
*   **Pandas**: Voor het structureren en manipuleren van data in rijen en kolommen.
*   **OpenAI API, Anthropic API, Google's API**: Verschillende API's van model providers.
*   **Instructor**: Wordt gebruikt om gestructureerde uitvoer te halen uit modellen.
*   **PG Vector**: Vector databases voor het opslaan en ophalen van context.
*   **PiMuPDF, PyPDF2**: Libraries om informatie te extraheren uit documenten of PDFs.
*   **Jinja**: Een templating engine voor Python, handig voor het bouwen van dynamische prompts.

#### Links
* [OpenWebUI](https://github.com/open-webui/open-webui)
   * Open WebUI is an extensible, feature-rich, and user-friendly self-hosted AI platform designed to operate entirely offline. It supports various LLM runners like Ollama and OpenAI-compatible APIs, with built-in inference engine for RAG, making it a powerful AI deployment solution.
* [LiteLLM](https://github.com/BerriAI/litellm)
   * Call all LLM APIs using the OpenAI format [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc.]
:::

::: section
### Structured output
Structured Output betekent dat je de AI (zoals een Large Language Model, LLM) niet zomaar een lap tekst laat schrijven, maar dat je **expliciet vraagt om de output in een vast, voorspelbaar formaat**, zoals JSON, XML, of een Python-object.

#### Waarom is gestructureerde output wenselijk

**Ongestructureerde Output:**

  * **Vraag:** "Maak een lijst met wat we nodig hebben voor de lunch."
  * **Antwoord (Vrije Tekst):** "We hebben brood, kaas, en melk nodig. Oh ja, en vergeet de ham niet die we gisteren zagen."
  * **Probleem:** Dit is prima voor een mens, maar een computer (een app of een ander programma) kan deze vrije tekst niet makkelijk verwerken. Het moet de tekst analyseren om erachter te komen welk woord het item is en welk woord een opmerking.

**Gestructureerde Output (JSON):**

  * **Vraag:** "Maak een boodschappenlijstje, maar geef het terug als een JSON-lijst met alleen de naam en de hoeveelheid."
  * **Antwoord (JSON):**
    ```json
    [
      {"item": "Brood", "hoeveelheid": "1 stuk"},
      {"item": "Kaas", "hoeveelheid": "200 gram"},
      {"item": "Melk", "hoeveelheid": "1 liter"},
      {"item": "Ham", "hoeveelheid": "150 gram"}
    ]
    ```
  * **Voordeel:** Nu is het supermakkelijk voor een Python-programma\! Je gebruikt gewoon de ingebouwde JSON-parser, en je hebt direct een bruikbare **lijst met objecten** in Python. Geen ingewikkelde tekstanalyse nodig.

#### Technieken voor Python-Programmeren met AI

In Python, wanneer je met een LLM API werkt (zoals die van Google, OpenAI, of Meta), zijn er twee hoofdmanieren om dit te bereiken:

1.  **Instructies in de Prompt (De Basis):**

      * Je voegt duidelijke, dwingende zinnen toe aan je prompt: "Je antwoord moet **uitsluitend** in JSON-formaat zijn." of "Gebruik dit XML-schema."
      * *Nadeel:* Dit is niet 100% betrouwbaar. Soms 'vergeet' de AI de instructie en voegt toch extra tekst of uitleg toe.

2.  **Schema's/Tools Gebruiken (De Professionele Manier):**

      * Dit is de beste manier. Je geeft de AI niet alleen de tekstprompt, maar ook een **formeel schema** (vaak met de **Pydantic**-bibliotheek in Python).
      * Je definieert in Python hoe de output eruit *moet* zien (bijvoorbeeld: een klasse `Product` met de velden `naam` (string) en `prijs` (float)).
      * De LLM API dwingt zichzelf af om de JSON-output te genereren die exact aan dat schema voldoet.

    ```python
    # Voorbeeld met een hypothetisch schema
    class Product(BaseModel):
        naam: str
        prijs: float
        is_op_voorraad: bool

    # Je roept de AI aan met dit schema
    # De output is gegarandeerd JSON die hieraan voldoet!
    ```
3. [Github repository met een voorbeeld van structured output op basis van python en pydantic](https://github.com/siewers32/structured_output)

Voor bedrijven is gestructureerde output geen luxe, maar vaak een **noodzaak** voor automatisering en betrouwbaarheid.

| Reden | Uitleg |
| :--- | :--- |
| **Automatisering** | **De 'Mensenloze' Pipeline:** In een bedrijf wordt data vaak van het ene systeem naar het andere doorgegeven (bijvoorbeeld: van een AI-samenvatting naar een database of een rapportage-tool). Als de input altijd hetzelfde formaat heeft, kan het verwerkingsprogramma (de Python-code) zonder menselijke tussenkomst blijven draaien. **Dit bespaart tijd en loonkosten.** |
| **Betrouwbaarheid** | **Geen Kapotte Systemen:** Als de AI plotseling een emoji of een extra zin in de output zet, crasht het volgende programma dat de JSON verwacht. Met structured output **garandeer je dat de data altijd valide is**, wat leidt tot minder fouten en minder 'downtime'. |
| **Uniformiteit** | **Standaardisatie:** Een bedrijf wil dat alle gegevens op dezelfde manier worden opgeslagen, ongeacht of de data uit een formulier, een database, of de AI komt. Gestructureerde data zorgt voor **consistentie** over alle afdelingen heen. |
| **Schaalbaarheid** | **Klaar voor Grote Hoeveelheden:** Als je duizenden taken per dag met AI moet uitvoeren, kun je niet elke output handmatig controleren. Doordat de output gestructureerd is, kunnen systemen **grote volumes data snel en foutloos** verwerken en opslaan. |

:::

::: section
### RAG

#### Wat betekent Retrieval Augmented Generation?

Het is een techniek waarbij een AI extra informatie opzoekt voordat hij een antwoord geeft. Je kunt het vergelijken met een student die een vraag krijgt en eerst even iets opzoekt op internet of in een boek, en daarna pas antwoord geeft.

#### Stap voor stap uitgelegd

#### 1. Je stelt een vraag
Bijvoorbeeld:  
*"Hoe werkt zonne-energie?"*

#### 2. De AI gaat op zoek (retrieval)
In plaats van meteen te antwoorden, zoekt de AI eerst naar betrouwbare teksten of documenten waar iets staat over zonne-energie. Dit kunnen dingen zijn als artikelen, websites, of een interne kennisbank.

#### 3. AI leest en haalt informatie op
De AI kiest stukjes tekst die het meest lijken op wat jij vroeg. Bijvoorbeeld een alinea uit een Wikipedia-pagina over zonne-energie.

#### 4. De AI maakt een antwoord (generation)
Nu gebruikt de AI die gevonden informatie om een nieuw antwoord te maken dat goed past bij jouw vraag.

#### 5. Je krijgt een slimmer antwoord
Omdat de AI eerst informatie heeft opgezocht, krijg je een antwoord dat vaak nauwkeuriger en actueler is dan wanneer de AI het helemaal uit z’n eigen "geheugen" haalt.


#### Simpele vergelijking
- Zonder RAG: De AI probeert alles te herinneren, zoals een student zonder aantekeningen.
- Met RAG: De AI zoekt het eerst even op, net zoals een student die in zijn boek kijkt voordat hij antwoordt.

#### Wanneer wordt dit gebruikt?
- Bij chatbots van klantenservice
- In zoekmachines met slimme AI-antwoorden
- Voor bedrijven die willen dat AI antwoorden geeft op basis van hun eigen documenten

#### Voorbeeldcode
* [RAG-code in python op github](https://github.com/siewers32/rag)

:::


::: section
### MCP
MCP staat voor **Model Context Protocol**. Het is een open en universeel protocol dat fungeert als een gestandaardiseerde communicatiebrug tussen AI-modellen (zoals grote taalmodellen of 'Large Language Models', LLM's) en externe tools, data en diensten.

Je kunt MCP zien als de **USB-C-poort voor AI-applicaties**: het zorgt voor een uniforme, plug-and-play-manier om AI te verbinden met de rest van de digitale wereld, waardoor AI-modellen niet meer 'geïsoleerd' werken.

#### Waarom MCP Nodig Is

AI-modellen zijn slim, maar ze zijn getraind op een vaste set data. Ze kunnen zelf geen acties ondernemen buiten hun eigen context. MCP lost dit op door AI-systemen toegang te geven tot:

1.  **Realtime Data:** De AI kan actuele informatie opvragen (zoals het weer, beurskoersen of de nieuwste bedrijfsdocumenten).
2.  **Externe Tools:** De AI kan acties uitvoeren in andere programma's (zoals een e-mail versturen, een taak aanmaken in een projectmanager of data ophalen uit een CRM-systeem).

Dit maakt de AI een **Agent** die autonoom taken kan uitvoeren en beslissingen kan nemen op basis van actuele en specifieke context.

#### De Componenten van MCP

MCP werkt met twee hoofdcomponenten:

| Component | Functie |
| :--- | :--- |
| **MCP Client** | Het AI-systeem of de applicatie die de hulp vraagt. Dit kan een AI-chatbot, een IDE (zoals VS Code), of een op AI gebaseerde workflowtool zijn. |
| **MCP Server** | De 'adapter' of 'connector' die de toegang tot een specifieke tool of databron regelt (bijvoorbeeld een GitHub-server of een database-server). De Server vertaalt de gestructureerde vraag van de AI naar een actie in de tool, en stuurt het resultaat terug. |

#### Hoe MCP Werkt met Docker

Docker is de ideale partner voor MCP omdat het de servers die je nodig hebt voor AI-integraties, **veilig en eenvoudig** isoleert en beheert.

MCP-servers hebben vaak specifieke runtime-vereisten (zoals een bepaalde versie van Python of Node.js), en ze moeten omgaan met gevoelige gegevens (API-sleutels). Docker lost deze problemen op:

#### 1. Isolatie en Beveiliging (Containers)

Voordat Docker moesten MCP-servers vaak direct op je lokale machine worden uitgevoerd, wat beveiligingsrisico's met zich meebracht (de AI kon in theorie toegang krijgen tot je lokale bestanden).

* **De Docker Oplossing:** Door elke MCP Server in een afzonderlijke **Docker Container** te plaatsen, creëer je een **sandbox-omgeving**. De AI krijgt alleen toegang tot wat je expliciet deelt, zoals een database of een specifieke API, en niet tot het hele hostsysteem.
* **Geheim Beheer:** Docker kan gevoelige gegevens (API-sleutels, tokens) veilig opslaan als **Docker Secrets**, waardoor ze niet als platte tekst in configuratiebestanden hoeven te staan.

#### 2. Eenvoudige Deployment en Beheer (Toolkit)

Docker heeft een set tools ontwikkeld, waaronder de **Docker MCP Toolkit** en **MCP Catalogus**, om het opzetten van MCP-servers te vereenvoudigen.

* **Docker MCP Catalogus:** Dit is een gecentraliseerde hub (vaak geïntegreerd in Docker Desktop) waar ontwikkelaars geverifieerde, kant-en-klare MCP Servers (voor GitHub, Jira, Perplexity, etc.) kunnen vinden. Je hoeft de code niet zelf te bouwen.
* **Docker MCP Toolkit:** Met deze tool kun je de gevonden MCP Servers direct installeren, configureren en starten met de vertrouwde Docker-commando's of via een simpele klik in Docker Desktop. Het beheert de volledige levenscyclus van de server.
* **Uniforme Interface (Gateway):** Docker stelt een **MCP Gateway** in, een enkel toegangspunt waar alle MCP Clients (zoals de AI-assistent in je IDE) verbinding mee maken. Deze gateway regelt de communicatie met alle afzonderlijk draaiende MCP Servers, wat de configuratie voor de gebruiker vereenvoudigt.

:::